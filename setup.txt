RAG API â€“ Local Setup Guide
==========================

This project uses:
- Python (with virtual environment)
- FastAPI
- ChromaDB
- Ollama (local LLM server)

Follow the steps in order.


--------------------------------------------------
1. PREREQUISITES
--------------------------------------------------

Make sure these are installed on your system:

- Python 3.9 or higher
  Check:
    python --version

- Git
  Check:
    git --version


--------------------------------------------------
2. CLONE THE PROJECT
--------------------------------------------------

Clone the repository and enter the project folder:

    git clone <your-repo-url>
    cd <project-folder>


--------------------------------------------------
3. CREATE & ACTIVATE PYTHON VIRTUAL ENVIRONMENT
--------------------------------------------------

Create virtual environment:

    python -m venv venv


Activate it:

macOS / Linux:
    source venv/bin/activate

Windows (PowerShell):
    venv\Scripts\activate


After activation, you should see:
    (venv) in your terminal


--------------------------------------------------
4. INSTALL PYTHON DEPENDENCIES
--------------------------------------------------

Install required packages:

    pip install -r requirements.txt


If requirements.txt is missing, create it with:

    fastapi
    uvicorn[standard]
    chromadb
    ollama
    python-dotenv


--------------------------------------------------
5. INSTALL OLLAMA
--------------------------------------------------

Ollama is required to run the LLM locally.

macOS:
    Install Homebrew (if not installed)
    https://brew.sh

    Then run:
    brew install --cask ollama

Windows:
    Download installer from:
    https://ollama.com/download/windows

    Run installer and ensure Ollama is added to PATH


--------------------------------------------------
6. START OLLAMA SERVER
--------------------------------------------------

macOS:
    Open Ollama app (from Applications)
    OR
    open -a "Ollama"

Windows:
    ollama serve


Keep Ollama running in the background.


Verify Ollama is running:

    curl http://localhost:11434

You should see a response confirming Ollama is running.


--------------------------------------------------
7. DOWNLOAD REQUIRED MODEL
--------------------------------------------------

Pull the TinyLLaMA model:

    ollama pull tinyllama


You only need to do this once.


--------------------------------------------------
8. RUN THE RAG API
--------------------------------------------------

Start the FastAPI server:

    uvicorn app:app --reload


If your main file is different, replace `app` accordingly.

Example:
    uvicorn main:app --reload


--------------------------------------------------
9. TEST THE API
--------------------------------------------------

Open browser and visit:

    http://127.0.0.1:8000/docs

This opens FastAPI Swagger UI.


--------------------------------------------------
10. COMMON ISSUES
--------------------------------------------------

- venv not activating:
  Make sure you are using the correct command for your OS.

- Ollama connection error:
  Ensure `ollama serve` is running.

- Port already in use:
  Stop other services using port 8000 or change port:
    uvicorn app:app --reload --port 8080


--------------------------------------------------
SETUP COMPLETE ðŸŽ‰
--------------------------------------------------